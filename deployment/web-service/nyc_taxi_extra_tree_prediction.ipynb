{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a42474a",
   "metadata": {},
   "source": [
    "---\n",
    "### Import Python Libraries\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acc1f814-9999-41fe-957e-359fbb42ae54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import mlflow\n",
    "import xgboost as xgb\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from xgboost import XGBRegressor  # Import XGBoost\n",
    "from sklearn.linear_model import #LinearRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd94aa15",
   "metadata": {},
   "source": [
    "---\n",
    "### Define Dataset Paths\n",
    "---\n",
    "1. Training to be done with data recorded in the month of **JULY**\n",
    "2. Validation to be done with data recorded in the month of **AUGUST**\n",
    "3. Testing to be done with data recorded in the month of **SEPTEMBER**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da01f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the training filepath\n",
    "filename_jul_train = './dataset/yellow_tripdata_2023-08.parquet'\n",
    "filename_aug_validate = './dataset/yellow_tripdata_2023-08.parquet'\n",
    "filename_sept_test = './dataset/yellow_tripdata_2023-09.parquet'\n",
    "model_output_filepath = './model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70da886",
   "metadata": {},
   "source": [
    "---\n",
    "### MLflow Configuration for Experiment Tracking\n",
    "---\n",
    "MLflow, is an open-source platform for managing the end-to-end machine learning lifecycle. \n",
    "It involves setting the tracking URI and experiment name.\n",
    "\n",
    "**THE TRACKING URI**\n",
    "\n",
    "1. The tracking URI is the location where MLflow logs and stores information about experiments, runs, and artifacts.\n",
    "2. In this case, it is set to a local server running at http://127.0.0.1:5000. \n",
    "3. This means that MLflow will store its data in a SQLite database at that location.\n",
    "\n",
    "**THE EXPERIMENT NAME**\n",
    "\n",
    "1. In MLflow, an experiment is a named collection of runs, and it helps organize and track different machine learning workflows. \n",
    "2. The `mlflow.set_experiment` function is used to set the current experiment to the one specified by experiment_name. \n",
    "3. This means that any subsequent runs or logging within this script will be associated with the \"POC_NYC-Yellow-Taxi-Experiment\" experiment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3458bf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/12/05 18:19:23 INFO mlflow.tracking.fluent: Experiment with name 'NYC-Yellow-Taxi-Experiment-POC' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/4', creation_time=1701818363975, experiment_id='4', last_update_time=1701818363975, lifecycle_stage='active', name='NYC-Yellow-Taxi-Experiment-POC', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the tracking URI to use SQLite as the backend store\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "\n",
    "# Set the experiment name mlflow\n",
    "EXPERIMENT_NAME = \"NYC-Yellow-Taxi-Experiment-POC\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b5e56b",
   "metadata": {},
   "source": [
    "---\n",
    "### Function:Data Preprocessing\n",
    "---\n",
    "This function reads a DataFrame from either a CSV file or a Parquet file, performs specific data preprocessing steps related to datetime, duration, filtering, and column manipulation, and returns the resulting DataFrame\n",
    "\n",
    "Following functions are performed under data pre-processing:\n",
    "1. File Format Check\n",
    "2. Datetime Conversion:\n",
    "3. Calculate Duration in Minutes\n",
    "4. Filter Duration\n",
    "5. Categorical Conversion\n",
    "6. Create Combined Location ID\n",
    "7. Return Processed DataFrame\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d4ac396-f2cf-44c7-9320-3fcc8773e21f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_preprocess(filename):\n",
    "    # Check file format and read DataFrame accordingly\n",
    "    if filename.endswith('.csv'):\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        # Convert datetime columns to pandas datetime objects\n",
    "        df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n",
    "        df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "        \n",
    "    elif filename.endswith('.parquet'):\n",
    "        df = pd.read_parquet(filename)\n",
    "        \n",
    "    # Calculate trip duration in minutes\n",
    "    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "    \n",
    "    # Filter trips based on duration (between 1 and 60 minutes)\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "    \n",
    "    # Convert selected columns to string type for categorical representation\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    \n",
    "    # Create a new column 'PU_DO' by combining pickup and dropoff location IDs\n",
    "    df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']\n",
    "    \n",
    "    # Return the processed DataFrame\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5554eb46",
   "metadata": {},
   "source": [
    "---\n",
    "### Function: Distribution Plotting Function for Comparing Predicted and Actual Values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25f7beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(y_train, y_pred, model_class):\n",
    "    # Plot the distribution of predicted values using Seaborn\n",
    "    print('====================================================================')\n",
    "    print(f'\\nplotting predicted and actual values for the model :{model_class}')\n",
    "    sns.distplot(y_pred, label='prediction_linear_regression')\n",
    "\n",
    "    # Plot the distribution of actual values using Seaborn\n",
    "    sns.distplot(y_train, label='actual')\n",
    "\n",
    "    # Display the legend to differentiate between the two distributions\n",
    "    plt.legend()\n",
    "\n",
    "# Example usage:\n",
    "# plot_graph(y_train, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7102201b",
   "metadata": {},
   "source": [
    "---\n",
    "### Function: MLflow Integration for Logging, Training, and Evaluating Machine Learning Models\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf427dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_model(model_class, X_train, y_train, X_val, y_val):\n",
    "    # Start an MLflow run\n",
    "    with mlflow.start_run():\n",
    "        # Log parameters, including file paths and the model name\n",
    "        mlflow.log_params({\n",
    "            \"train-data-path\": f\"{filename_jul_train}\",\n",
    "            \"valid-data-path\": f\"{filename_aug_validate}\",\n",
    "            \"model-name\": f\"{model_class}\"\n",
    "        })\n",
    "\n",
    "        # Log a preprocessor artifact\n",
    "        mlflow.log_artifact(\"models/preprocessor.b\", artifact_path=\"preprocessor\")\n",
    "\n",
    "        # Train the model\n",
    "        mlmodel = model_class()\n",
    "        mlmodel.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions and calculate RMSE\n",
    "        y_pred = mlmodel.predict(X_val)\n",
    "        rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "\n",
    "        # Plot a distribution graph of actual vs predicted values\n",
    "        plot_graph(y_train, y_pred, model_class)\n",
    "\n",
    "        # Log the RMSE metric\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "        # Log the trained model\n",
    "        mlflow.sklearn.log_model(mlmodel, \"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e11f4f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(X_train, y_train, X_val, y_val):\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    # List of regression models to iterate over\n",
    "    regression_models = [\n",
    "        RandomForestRegressor, \n",
    "        GradientBoostingRegressor, \n",
    "        ExtraTreesRegressor, \n",
    "        LinearSVR,\n",
    "        XGBRegressor, \n",
    "        LinearRegression  # Include LinearRegression\n",
    "    ]\n",
    "\n",
    "    for model_class in regression_models:\n",
    "        train_and_log_model(model_class, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57009d75",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Main Function\n",
    "---\n",
    "---\n",
    "#### Data Preprocessing and Dataset Initialization\n",
    "---\n",
    "1. The `data_preprocess` function on three different filenames corresponding to training, validation, and test datasets. \n",
    "\n",
    "2. It then prints the lengths of the resulting DataFrames for the training and validation datasets.\n",
    "\n",
    "---\n",
    "#### Feature Transformation using DictVectorizer\n",
    "---\n",
    "1. The lists `categorical` and `numerical` define the features to be treated as categorical and numerical, respectively.\n",
    "\n",
    "2. The `DictVectorizer` is initialized.\n",
    "\n",
    "3. The training data is converted to a dictionary of records (`train_dicts`) and then transformed into a sparse matrix (`X_train`) using the `fit_transform` method of `DictVectorizer`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09266c6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/shreyajaiswal/Library/CloudStorage/OneDrive-NortheasternUniversity/Projects/MLOps-NYC-Taxi/experiment_tracking/NYC_Taxi_Prediction.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shreyajaiswal/Library/CloudStorage/OneDrive-NortheasternUniversity/Projects/MLOps-NYC-Taxi/experiment_tracking/NYC_Taxi_Prediction.ipynb#X65sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Convert training data to a dictionary of records and then transform it into a sparse matrix\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shreyajaiswal/Library/CloudStorage/OneDrive-NortheasternUniversity/Projects/MLOps-NYC-Taxi/experiment_tracking/NYC_Taxi_Prediction.ipynb#X65sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m train_dicts \u001b[39m=\u001b[39m df_train[categorical \u001b[39m+\u001b[39m numerical]\u001b[39m.\u001b[39mto_dict(orient\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrecords\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/shreyajaiswal/Library/CloudStorage/OneDrive-NortheasternUniversity/Projects/MLOps-NYC-Taxi/experiment_tracking/NYC_Taxi_Prediction.ipynb#X65sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m X_train \u001b[39m=\u001b[39m dv\u001b[39m.\u001b[39;49mfit_transform(train_dicts)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shreyajaiswal/Library/CloudStorage/OneDrive-NortheasternUniversity/Projects/MLOps-NYC-Taxi/experiment_tracking/NYC_Taxi_Prediction.ipynb#X65sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Convert validation data to a dictionary of records and then transform it into a sparse matrix\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shreyajaiswal/Library/CloudStorage/OneDrive-NortheasternUniversity/Projects/MLOps-NYC-Taxi/experiment_tracking/NYC_Taxi_Prediction.ipynb#X65sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m val_dicts \u001b[39m=\u001b[39m df_validate[categorical \u001b[39m+\u001b[39m numerical]\u001b[39m.\u001b[39mto_dict(orient\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrecords\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/sklearn/feature_extraction/_dict_vectorizer.py:316\u001b[0m, in \u001b[0;36mDictVectorizer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    293\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    294\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Learn a list of feature name -> indices mappings and transform X.\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \n\u001b[1;32m    296\u001b[0m \u001b[39m    Like fit(X) followed by transform(X), but does not require\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39m        Feature vectors; always 2-d.\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(X, fitting\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/sklearn/feature_extraction/_dict_vectorizer.py:268\u001b[0m, in \u001b[0;36mDictVectorizer._transform\u001b[0;34m(self, X, fitting)\u001b[0m\n\u001b[1;32m    265\u001b[0m indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfrombuffer(indices, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintc)\n\u001b[1;32m    266\u001b[0m shape \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(indptr) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(vocab))\n\u001b[0;32m--> 268\u001b[0m result_matrix \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39;49mcsr_matrix(\n\u001b[1;32m    269\u001b[0m     (values, indices, indptr), shape\u001b[39m=\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49mdtype\n\u001b[1;32m    270\u001b[0m )\n\u001b[1;32m    272\u001b[0m \u001b[39m# Sort everything if asked\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m fitting \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/scipy/sparse/_compressed.py:65\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     maxval \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(shape)\n\u001b[0;32m---> 65\u001b[0m idx_dtype \u001b[39m=\u001b[39m get_index_dtype((indices, indptr),\n\u001b[1;32m     66\u001b[0m                             maxval\u001b[39m=\u001b[39;49mmaxval,\n\u001b[1;32m     67\u001b[0m                             check_contents\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     69\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(indices, copy\u001b[39m=\u001b[39mcopy,\n\u001b[1;32m     70\u001b[0m                         dtype\u001b[39m=\u001b[39midx_dtype)\n\u001b[1;32m     71\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindptr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(indptr, copy\u001b[39m=\u001b[39mcopy, dtype\u001b[39m=\u001b[39midx_dtype)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/scipy/sparse/_sputils.py:190\u001b[0m, in \u001b[0;36mget_index_dtype\u001b[0;34m(arrays, maxval, check_contents)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39melif\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(arr\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger):\n\u001b[0;32m--> 190\u001b[0m     maxval \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39;49mmax()\n\u001b[1;32m    191\u001b[0m     minval \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mmin()\n\u001b[1;32m    192\u001b[0m     \u001b[39mif\u001b[39;00m minval \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m int32min \u001b[39mand\u001b[39;00m maxval \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m int32max:\n\u001b[1;32m    193\u001b[0m         \u001b[39m# a bigger type not needed\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.8/site-packages/numpy/core/_methods.py:41\u001b[0m, in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_amax\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m           initial\u001b[39m=\u001b[39m_NoValue, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m umr_maximum(a, axis, \u001b[39mNone\u001b[39;49;00m, out, keepdims, initial, where)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Preprocess the training data\n",
    "    df_train = data_preprocess(filename_jul_train)\n",
    "\n",
    "    # Preprocess the validation data\n",
    "    df_validate = data_preprocess(filename_aug_validate)\n",
    "\n",
    "    # Preprocess the test data\n",
    "    df_test = data_preprocess(filename_sept_test)\n",
    "\n",
    "    # Print the lengths of the training and validation DataFrames\n",
    "    len(df_train), len(df_validate)\n",
    "\n",
    "    # Load your data (X_train, y_train, X_val, y_val)\n",
    "    # Define categorical and numerical features : independant features\n",
    "    categorical = ['PU_DO']\n",
    "    numerical = ['trip_distance']\n",
    "\n",
    "    # Initialize a DictVectorizer\n",
    "    dv = DictVectorizer()\n",
    "\n",
    "    # Convert training data to a dictionary of records and then transform it into a sparse matrix\n",
    "    train_dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "    X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "    # Convert validation data to a dictionary of records and then transform it into a sparse matrix\n",
    "    val_dicts = df_validate[categorical + numerical].to_dict(orient='records')\n",
    "    X_val = dv.transform(val_dicts)\n",
    "\n",
    "    # Convert test data to a dictionary of records and then transform it into a sparse matrix\n",
    "    test_dicts = df_test[categorical + numerical].to_dict(orient='records')\n",
    "    X_test = dv.transform(test_dicts)\n",
    "\n",
    "    #dependant feature/ target variable\n",
    "\n",
    "    target = 'duration'\n",
    "\n",
    "    y_train = df_train[target].values\n",
    "    y_val = df_validate[target].values\n",
    "    y_test = df_test[target].values\n",
    "\n",
    "    # Call the main function with your data\n",
    "    main(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3dc9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'{model_output_filepath}/lr-v-1.bin', 'wb') as f_out:\n",
    "#     pickle.dump((dv, lr), f_out)\n",
    "#     print(\"sucessfully recorded current model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a280542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mlflow.start_run():\n",
    "\n",
    "#     mlflow.set_tag(\"developers\",\"mlops-nyc-yellow-taxi\")\n",
    "#     mlflow.log_param(\"train-data_path\",f\"{filename_aug_train}\")\n",
    "#     mlflow.log_param(\"validate-data_path\",f\"{filename_sept_validate}\")\n",
    "\n",
    "#     alpha = 0.01\n",
    "\n",
    "#     mlflow.log_param(\"alpha\",f\"{alpha}\")\n",
    "#     y_pred_lr, rmse =model_linear_regression(X_train,y_train)\n",
    "\n",
    "#     mlflow.log_metric(\"rmse\", rmse)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672aa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mlflow.start_run():\n",
    "\n",
    "#     mlflow.set_tag(\"developers\",\"shreya\",\"nyc-yellow-taxi\",\"mlops\")\n",
    "#     mlflow.log_param(\"train-data_path\",f\"{filename_aug_train}\")\n",
    "#     mlflow.log_param(\"validate-data_path\",f\"{filename_sept_validate}\")\n",
    "\n",
    "#     alpha = 0.01\n",
    "\n",
    "#     mlflow.log_param(\"alpha\",f\"{alpha}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c997406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = xgb.DMatrix(X_train, label=y_train)\n",
    "# valid = xgb.DMatrix(X_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090f135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def xgboost(params):\n",
    "#     with mlflow.start_run():\n",
    "#         mlflow.set_tag(\"model\", \"xgboost\")\n",
    "#         mlflow.log_params(params)\n",
    "#         booster = xgb.train(\n",
    "#             params=params,\n",
    "#             dtrain=train,\n",
    "#             num_boost_round=1,\n",
    "#             evals=[(valid, 'validation')],\n",
    "#             early_stopping_rounds=5\n",
    "#         )\n",
    "#         y_pred = booster.predict(valid)\n",
    "#         rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "#         mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "#     return {'loss': rmse, 'status': STATUS_OK}\n",
    "\n",
    "# search_space = {\n",
    "#     'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "#     'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "#     'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "#     'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "#     'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "#     'objective': 'reg:linear',\n",
    "#     'seed': 42\n",
    "# }\n",
    "\n",
    "# best_result = fmin(\n",
    "#     fn=xgboost,\n",
    "#     space=search_space,\n",
    "#     algo=tpe.suggest,\n",
    "#     max_evals=2,\n",
    "#     trials=Trials()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d042525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_space = {\n",
    "#     'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "#     'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "#     'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "#     'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "#     'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "#     'objective': 'reg:linear',\n",
    "#     'seed': 42\n",
    "# }\n",
    "\n",
    "# best_result = fmin(\n",
    "#     fn=xgboost,\n",
    "#     space=search_space,\n",
    "#     algo=tpe.suggest,\n",
    "#     max_evals=2,\n",
    "#     trials=Trials()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc39c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "    \n",
    "# }\n",
    "\n",
    "# mlflow.xgboost.autolog()\n",
    "\n",
    "# with mlflow.start_run():\n",
    "#     mlflow.set_tag()\n",
    "#     mlflow.log_param()\n",
    "\n",
    "#     booster = xgb.train(\n",
    "#         booster = xgb.train(\n",
    "#         params=params,\n",
    "#         dtrain=train,\n",
    "#         num_boost_round=1000,\n",
    "#         evals=[(valid, 'validation')],\n",
    "#         early_stopping_rounds=50\n",
    "# )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e5400-1379-4fb2-9b38-1ff757d04909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Fitting the linear regression model to the training data \n",
    "# def model_linear_regression(X_train,y_train):\n",
    "#     lr = LinearRegression()\n",
    "#     lr.fit(X_train, y_train)\n",
    "\n",
    "#     #using the trained model to predict outcome of the validation data\n",
    "#     y_pred = lr.predict(X_val)\n",
    "\n",
    "#     print(\"The mean square error is --> \", mean_squared_error(y_val, y_pred, squared=False))\n",
    "#     return y_pred, mean_squared_error(y_val, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882933cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8867dc-5778-49e7-90ed-cba2ad5ea673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc505ea-a19b-4be8-9e0d-f1e4358c9bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb3ef57d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09f0e736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "738439a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f6d1712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b7b7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ccbbb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe7f18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
